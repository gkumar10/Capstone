Coursera Capstone: Next Word Prediction
========================================================
author: Gaurav Kumar
date: Friday August 21, 2015

Executive Summary
========================================================
<font size=5>Most on-screen touch keyboards in mobile apps use word completion to make typing easier and faster.

Next few slides detail my attempt to <i>predict</i> the next word instead of partial word completion. I will explain the methodology behind prediction and how it can get better.

<u>App Description and Instructions:</u>

The app will suggest best choice of next word for input text. The app will also suggest other available word choices depending on your preference.

To use the app - follow the instructions:

1. Type a phrase with at least 1 word.

2. Select the checkbox if you want to see other available word choices.

3. Click 'Predict Next Word' button. 

Shinyapps link: https://gkumar10.shinyapps.io/capstone
</font>

Methodology Used To Predict
========================================================
<font size=6>I used a large English language corpus of 95Million+ words derived from Blogs, News article and Twitter feeds. 

After pre-processing - no punctuations, no numeric digits, convert to lower case etc - I created 1-gram, 2-gram and 3-gram data sets.

The algorithm takes written text as input and looks for match in n-gram for highest maximum likelihood estimation (MLE). If no evidence in highest n-gram, algorithm backs-off to n-1 gram.

The 1-grams are useful when there is no evidence in higher order grams - the 2-gram and 3-gram in my algorithm. At the unigram level, I calculate probability of unseen grams using Good Turing Smoothing to return words with higher probability. I used formulas learnt from video lectures of Natural Language Processing class in Coursera as referenced in 'References' slide.
</font>

Observations and Next Steps
========================================================
The algorithm can obviously get a lot better. 

In random tests conducted with the help of friends and family - the algorithm did a better job of predicting early parts of a sentence (~60% correct) than the later part of sentence (~25%). This could be due to long-distance dependencies - as the number of words in a sentence increases, a lot of topical context is needed to increase likelihood of successful match.

If I were to continue building this algorithm. I would explore adding local app context to the corpus. For ex. context from Facebook feeds, Google Autocomplete, Whatsapp etc. These are just random ideas at this time.


References
========================================================
I refered the following sources to complete my project: 
- http://www.cs.sfu.ca/~anoop/teaching/CMPT-413-Spring-2014/smooth.pdf
- http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf
- Coursera Natural Language Processing video lectures https://class.coursera.org/nlp
- Ideas from Coursera Capstone Project Discussion Forums.
