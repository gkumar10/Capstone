Coursera Capstone - Next Word Prediction
========================================================
author: Gaurav Kumar
date: Friday August 14, 2015

Executive Summary
========================================================
Most on-screen keyboards in smartphone and tablet apps use word completion to make typing easier and faster.

The next few slides detail my attempt to go a step further and <i>predict</i> the next word without the use of partial word completion. I will explain the methodology behind prediction and how it can get better.

Link to the app: https://gkumar10.shinyapps.io/capstone

Snapshot:



Methodology Used To Predict
========================================================
I used a large English language corpus of 95Million+ words and phrases derived from Blogs, News article and Twitter feeds. 

After pre-processing - remove punctuations, remove numeric digits, convert to lower case etc - I created 1-gram, 2-gram and 3-gram data sets.

The algorithm takes the written text as input and looks for match in n-gram with highest maximum likelihood estimation (MLE). If no evidence in highest n-gram, algorithm backs-off to n-1-gram.

The unigram is useful exactly when there is no evidence in higher order grams - the 2-gram and 3-gram in my algorithm. At the unigram level, I calculate probability using Katz Back Off model with discount of 0.2, Add-One Smoothing and Good Turing Smoothing to return the top 6 words with highest probability.

Observations and Next Steps
========================================================
The algorithm can obviously get a lot better. 

In some random tests conducted with the help of friends and family - the algorithm did a better job of predicting early parts of a sentence (~60% correct) than the later part of the sentence (~25%). This could be due to the fact that as the number of words in a sentence increase a lot of topical context is needed.

If I were to continue building this algorithm. I would like to explore adding local app context to the corpus. For ex.context from Facebook feeds, Google Autocomplete, Whatsapp etc. These are just randome ideas at this time.


References
========================================================
I refered the following sources to complete my project: 
- http://www.cs.sfu.ca/~anoop/teaching/CMPT-413-Spring-2014/smooth.pdf
- http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf
- Coursera Natural Language Processing Week1 video lectures https://class.coursera.org/nlangp-001/lecture
- Ideas from Coursera Capstone Project Discussion Forums.
